![Doctorate Thesis](https://img.shields.io/badge/Doctorate-Thesis-ffb6c1)
![University: Paris 8](https://img.shields.io/badge/University-Paris%208-red)
![deep: learning](https://img.shields.io/badge/deep-learning-blue)
![python](https://img.shields.io/badge/python-brightgreen)
![Contributors](https://img.shields.io/badge/contributor-1-orange)
![Stars](https://img.shields.io/github/stars/Fab16BSB/These?color=orange)
![Fork](https://img.shields.io/github/forks/Fab16BSB/These?color=orange)
![Watchers](https://img.shields.io/github/watchers/Fab16BSB/These?color=orange)

## üåç Versiones multiling√ºes del README

| üá´üá∑ Fran√ßais | üá¨üáß English | üá™üá∏ Espa√±ol |
|-------------|------------|------------|
| [README.fr.md](./README.fr.md) | [README.md](./README.md) | ¬°Est√°s aqu√≠! |

---

# Detecci√≥n de anomal√≠as en tiempo real en un flujo de v√≠deo

## üìå Contexto  

Realic√© una **tesis CIFRE** (Convenci√≥n Industrial de Formaci√≥n por la Investigaci√≥n) con una duraci√≥n de **3 a√±os** (del 4 de diciembre de 2019 al 4 de diciembre de 2022), titulada **_Detecci√≥n de anomal√≠as en tiempo real en un flujo de v√≠deo_**.  

Durante este per√≠odo, ocup√© el puesto de **investigador** en la startup [**Othello** üöÄ](http://www.othello.group), lo que me permiti√≥ llevar a cabo mis trabajos de investigaci√≥n directamente en la empresa, mientras estaba supervisado por el **Laboratorio de Inteligencia Artificial y Sem√°ntica de Datos (LIASD)** üß† de la Universidad **Paris 8** üìö, favoreciendo as√≠ la transferencia de conocimientos entre el mundo industrial y la investigaci√≥n ü§ù.

---

## üéØ Objetivo  

El objetivo de este proyecto era desarrollar una soluci√≥n de **detecci√≥n de anomal√≠as mediante inteligencia artificial** ü§ñ, en el marco de la convocatoria destinada a **garantizar la seguridad de los Juegos Ol√≠mpicos de Par√≠s 2024** üèÖ.  
Entre las restricciones impuestas:  

- El modelo deb√≠a funcionar **en tiempo real** para permitir una intervenci√≥n r√°pida y garantizar as√≠ la seguridad de las personas involucradas.  
- Los recursos disponibles eran limitados: **sin acceso a un servidor GPU** ni a **c√°maras de vigilancia reales**, lo que aument√≥ el desaf√≠o t√©cnico del proyecto.  
- El modelo deb√≠a ser **ligero y optimizado**, de manera que pudiera funcionar sin necesidad de un servidor GPU potente, lo cual era esencial dadas las limitaciones de recursos.  

---

## üí° Postulado  

Como seres humanos, podemos identificar f√°cilmente una situaci√≥n an√≥mala en una escena o en un video:  
- un **incendio** gracias a la presencia de humo o llamas üî•  
- una **pelea** observando las interacciones entre varias personas üë•  
- un **accidente de tr√°fico** vigilando los veh√≠culos üöó  
- un **peligro** potencial al detectar un arma blanca o de fuego üó°Ô∏èüî´  
- Adem√°s, un accidente de tr√°fico **no puede ocurrir** si ning√∫n veh√≠culo es visible en la pantalla.

Estas observaciones me inspiraron a combinar en este proyecto:  
1. **An√°lisis espacial**: detecci√≥n de objetos y poses humanas potencialmente sospechosas  
2. **An√°lisis temporal**: seguimiento de las acciones de los objetos e individuos a lo largo del tiempo  

El objetivo es crear una arquitectura que combine **an√°lisis espacial y temporal**, reproduciendo de manera simplificada la forma en que los humanos interpretan una escena observando los objetos y su din√°mica para comprender la situaci√≥n en su conjunto, con el fin de evaluar si este enfoque puede superar los m√©todos cl√°sicos.

---

## ‚öôÔ∏è Entorno t√©cnico  

Todo el proyecto se desarroll√≥ en un ordenador port√°til proporcionado por la empresa, equipado con los siguientes componentes:  
- **Memoria RAM:** 32 GB  
- **Procesador:** Intel Core i9, 16 n√∫cleos a 2,3 GHz  
- **Tarjeta gr√°fica:** Nvidia GeForce RTX 2080 con 8 GB de RAM dedicada  

En cuanto al software, el desarrollo y entrenamiento de los modelos se realiz√≥ con:  
- **Python**  
- **Keras** para el dise√±o y entrenamiento de los modelos de inteligencia artificial

---

## üìä Conjunto de datos  

### 1Ô∏è‚É£ Videos para la detecci√≥n de anomal√≠as

Ning√∫n conjunto de datos p√∫blico era suficientemente adecuado para entrenar nuestro modelo de manera eficaz:  
- Falta de volumen y variedad  
- Calidad insuficiente  
- Presencia de anomal√≠as que no representan consecuencias directas sobre la seguridad de las personas involucradas  

Por lo tanto, dise√±amos nuestro propio conjunto de datos.

‚ö†Ô∏è *Este conjunto de datos es propietario y no puede ser compartido ni difundido.* ‚ö†Ô∏è

#### üè∑Ô∏è Clases y distribuci√≥n

El conjunto de datos est√° compuesto por **4 clases**:  
- **Fight** (pelea)  
- **Shooting** (disparo)  
- **Fire** (incendio)  
- **Normal** (sin problemas detectados)  

Est√° **desequilibrado**, ya que algunas anomal√≠as son m√°s raras que otras. Su distribuci√≥n es la siguiente:  

![Distribuci√≥n de clases](images/dataset_distribution.png)  

| Clase     | Videos de entrenamiento (n) | Duraci√≥n entrenamiento | Videos de validaci√≥n (n) | Duraci√≥n validaci√≥n |
|-----------|-----------------------------|----------------------|--------------------------|-----------------------|
| Fight     | 587                         | 0h50                 | 391                      | 0h31                  |
| Fire      | 237                         | 3h40                 | 61                       | 0h46                  |
| Shooting  | 247                         | 0h17                 | 64                       | 0h08                  |

#### üñºÔ∏è Preprocesamiento y aumento de datos

- Cada video que contiene una anomal√≠a fue **recortado manualmente** para conservar solo la porci√≥n relevante.  
- Los videos se cargaron mediante un **video generator** (ver [repo GitHub](lien_vers_repo)) para formar **secuencias de 20 im√°genes de tama√±o 112x122**.  
- Se aplic√≥ un **aumento de datos realista** para simular situaciones plausibles:  
  - Modificaci√≥n de la **luminosidad**  
  - **Flip horizontal**  
  - **Zoom**  

### 2Ô∏è‚É£ Im√°genes para la detecci√≥n de objetos

De acuerdo con el postulado de **combinar la detecci√≥n de objetos con la detecci√≥n de anomal√≠as**, creamos un **segundo conjunto de datos basado en im√°genes**, enfocado en las entidades presentes en la pantalla y relacionadas con las anomal√≠as.

#### üè∑Ô∏è Clases y volumen

Las clases y vol√∫menes aproximados son:  
- **Armas de fuego**: ~10 000 im√°genes üî´  
- **Llamas**: >2 000 im√°genes üî•  
- **Personas**: cada individuo presente en las im√°genes üë§  

Tambi√©n se a√±adieron im√°genes **sin ninguna de estas clases** para el aprendizaje y la validaci√≥n, con el fin de manejar mejor las situaciones normales.

#### üñºÔ∏è Anotaci√≥n y uso

- Las im√°genes fueron **etiquetadas manualmente**.

---

## üß† Modelo

El sistema de detecci√≥n combina varias arquitecturas para procesar los flujos de video tanto en el **plano espacial** como en el **plano temporal**:  

- **CNN + RNN**: utilizado para el **an√°lisis temporal** de las secuencias de video.  
- **YOLOv3, YOLOv4, YOLOv7**: utilizados para el **an√°lisis espacial** de las im√°genes.  

![Diagrama del sistema](images/these_architecture1.png)  
![Diagrama del sistema](images/these_architecture2.png)  

### üîÑ Combinaci√≥n de modelos

Los modelos pueden combinarse de diferentes maneras:  
1. **Serie**: YOLO se utiliza para un **preprocesamiento espacial**, y sus resultados se env√≠an luego al CNN+RNN para el an√°lisis temporal.  
   ![Diagrama del sistema](images/modele_serie.png)
   
2. **Paralelo**: ambos modelos realizan la detecci√≥n simult√°neamente, y sus resultados se **fusionan** para producir la predicci√≥n final.  
![Diagrama del sistema](images/modele_parallele.png)

Tambi√©n se integra un **m√≥dulo de explicabilidad** para analizar ciertas predicciones, aunque **no funciona en tiempo real**.  

### üñ• CNN + RNN
![Diagrama CNN + RNN](images/modele.png)  

- **CNN**: VGG19  
- **RNN**: GRU con **1024 neuronas**  
- seguido de un **MLP de 3 capas** (1024 ‚Üí 512 ‚Üí 128 neuronas)  
  - **Dropout:** 50% en todas las capas  
  - **Regularizaci√≥n L2:** coeficiente 0.01  
- **Entrenamiento:**  
  - Entrenamiento por lotes (batch training) sobre nuestros videos  
  - **200 epochs**  
  - Optimizador: **Stochastic Gradient Descent (SGD)** con **learning rate 0.01**  

### üñº YOLO

Los modelos **YOLOv3, YOLOv4 y YOLOv7** fueron **reentrenados con nuestro conjunto de im√°genes**, permitiendo una detecci√≥n adaptada a las clases definidas en nuestro dataset.

---

## üìà Resultados

### üñºÔ∏è 1. Detecci√≥n de objetos

Los siguientes resultados muestran la detecci√≥n de los objetos clave en im√°genes extra√≠das de nuestros videos.  
Estos resultados ilustran la capacidad de nuestro modelo YOLO para localizar con precisi√≥n las entidades importantes para la seguridad.

![Ejemplo de detecci√≥n de arma de fuego](images/yolo_result.png)
![Ejemplo de detecci√≥n de llama](images/yolo_result_2.png)

---

### üé¨ 2. An√°lisis de video

Para ilustrar el rendimiento de nuestro enfoque, presentamos a continuaci√≥n varios ejemplos de an√°lisis de video.  
Cada secuencia combina la **detecci√≥n espacial** (personas, objetos, llamas, etc.) con un **an√°lisis temporal** basado en VGG-GRU, permitiendo resaltar las anomal√≠as en tiempo real.  

üì∏ **Capturas de pantalla**  
- Detecci√≥n de un inicio de incendio.  
![Ejemplo detecci√≥n de incendio (imagen)](images/incendie.png)  

- Detecci√≥n de un accidente de tr√°fico.  
![Ejemplo detecci√≥n de accidente (imagen)](images/accident.png)  

üé• **Ejemplos de videos**  
- [‚ñ∂Ô∏è Detecci√≥n de incendio en Notre-Dame (ver video)](https://youtu.be/Zg4AAycii1M)  
- [‚ñ∂Ô∏è Detecci√≥n de una pelea durante un partido de rugby (ver video)](https://youtu.be/vdMqYTSrXok) 

---

### üèÜ 3. Premio a la mejor demostraci√≥n

Video que ilustra nuestro primer resultado que recibi√≥ el **premio a la mejor demostraci√≥n en EGC 2022**.  

‚ñ∂Ô∏è [Ver video de demostraci√≥n](https://www.youtube.com/watch?v=EGHUEPMI4c8)

---

### üßê Explicabilidad del modelo  

Para comprender mejor el funcionamiento interno de nuestro sistema, hemos integrado un **m√≥dulo de explicabilidad**.  
Este cumple un doble prop√≥sito:  
- **Analizar las predicciones** del modelo para identificar sus razones.  
- **Ayudar en el ajuste de hiperpar√°metros** destacando las zonas o caracter√≠sticas m√°s influyentes.  

---

#### üîç Visualizaci√≥n de predicciones  
Estos ejemplos ilustran c√≥mo la explicabilidad resalta las regiones relevantes para la detecci√≥n de anomal√≠as:  

![Ejemplo de explicabilidad](images/explicabilite.png)  
![Ejemplo de explicabilidad](images/explicabilite_2.png)  

---

#### ‚öôÔ∏è Ayuda en la configuraci√≥n del modelo  
La explicabilidad tambi√©n se utiliz√≥ para analizar el comportamiento de las distintas capas (GRU y MLP), permitiendo afinar el dise√±o de la arquitectura y su parametrizaci√≥n.  

![Explicabilidad GRU](images/explicabilite_gru.png)  
![Explicabilidad MLP](images/explicabilite_mlp.png)

---

### üìä 5. Estad√≠sticas y rendimiento

Para evaluar nuestro enfoque, probamos diferentes **arquitecturas de combinaci√≥n** entre YOLO (para el an√°lisis espacial) y VGG-GRU (para el an√°lisis temporal).  
Se compararon dos configuraciones principales:  

1. **Modo serie**: las salidas de YOLO se utilizan directamente como entrada para VGG-GRU.  
2. **Modo paralelo**: YOLO y VGG-GRU realizan sus predicciones por separado, luego sus resultados se combinan para producir la decisi√≥n final.  

Las siguientes tablas presentan el rendimiento obtenido para estas dos configuraciones en t√©rminos de **precisi√≥n, recall, F1-score y matriz de confusi√≥n**.

#### Rendimiento de YOLO + VGG-GRU en modo serie

| M√©trica    | Exactitud | Precisi√≥n | Recall  | F1-Score |
|------------|-----------|-----------|---------|----------|
| Valor      | 87.3%     | 87.6%     | 87.3%   | 87.1%    |

#### Matriz de confusi√≥n en porcentaje para evaluaci√≥n en serie

| Truth \ Predicted | Pelea | Disparo | Incendio | Normal |
|------------------|-------|---------|----------|---------|
| **Pelea**        | 60.5% | 2.4%    | 1.3%     | 35.8%   |
| **Disparo**      | 10%   | 55.6%   | 14.8%    | 19.6%   |
| **Incendio**     | 15.5% | 10.6%   | 48%      | 25.9%   |
| **Normal**       | 3.4%  | 0.6%    | 1%       | 95%     |

#### Rendimiento de YOLO + VGG-GRU en modo paralelo

| M√©trica    | Exactitud | Precisi√≥n | Recall  | F1-Score |
|------------|-----------|-----------|---------|----------|
| Valor      | 78.42%    | 85.60%    | 78.42%  | 81.16%   |

#### Matriz de confusi√≥n en porcentaje para evaluaci√≥n en paralelo

| Truth \ Predicted | Pelea  | Disparo | Incendio | Normal |
|-------------------|--------|---------|----------|--------|
| **Pelea**         | 63.66% | 6.58%   | 1.93%    | 27.83% |
| **Disparo**       | 9.94%  | 66.06%  | 9.33%    | 14.67% |
| **Incendio**      | 13.66% | 15.73%  | 57.71%   | 12.9%  |
| **Normal**        | 7.43%  | 5.96%   | 3.98%    | 82.63% |

---

Finalmente, evaluamos ambos enfoques (paralelo y serie) desde el punto de vista del **tiempo de ejecuci√≥n** para verificar su aplicabilidad en condiciones reales.  
Los resultados muestran que:  
- El **modo paralelo** permite un procesamiento m√°s r√°pido, con un tiempo medio cercano al tiempo real para ciertos videos.  
- El **modo serie**, aunque m√°s preciso, tiene un costo temporal significativamente mayor.  

Las siguientes tablas resumen estas mediciones para diferentes videos de evaluaci√≥n.

#### Tiempo de ejecuci√≥n de YOLO + VGG-GRU en paralelo

| Duraci√≥n del video | FPS del video | Promedio de detecciones | Tiempo de procesamiento |
|--------------------|---------------|-----------------------|---------------------------|
| 16s                | 33            | 601ms                 | 15s                       |
| 44s                | 30            | 533ms                 | 35s                       |
| 9s                 | 30            | 994ms                 | 12s                       |
| 35s                | 30            | 1.1s                  | 57s                       |
| 23s                | 30            | 1s06                  | 35s                       |
| 1min 43            | 30            | 758ms                 | 116s (1min 56)            |
| 50s                | 30            | 826ms                 | 61s                       |
| 1min 05            | 30            | 886ms                 | 83s (1min 23)             |
| 2s                 | 30            | 847ms                 | 847ms                     |
| 9s                 | 30            | 870ms                 | 11s                       |
| 2s                 | 30            | 1s                    | 1s                        |

#### Tiempo de ejecuci√≥n de YOLO + VGG-GRU en serie

| Duraci√≥n del video | FPS del video | Promedio de detecciones | Tiempo de procesamiento |
|--------------------|---------------|-----------------------|---------------------------|
| 16s                | 33            | 1s                    | 26s                       |
| 44s                | 30            | 1s                    | 71s (1min 11)             |
| 9s                 | 30            | 1.5s                  | 20s                       |
| 35s                | 30            | 1.5s                  | 81s (1min 21)             |
| 23s                | 30            | 1.5s                  | 48s                       |
| 1min 43            | 30            | 1.2s                  | 193s (3min 13)            |
| 50s                | 30            | 1.3s                  | 102s (1min 42)            |
| 1min 05            | 30            | 1.4s                  | 134s (2min 14)            |
| 2s                 | 30            | 1.3s                  | 1.3s                      |
| 9s                 | 30            | 1.3s                  | 17s                       |
| 2s                 | 30            | 1.5s                  | 1.5s                      |

---

## ‚ö†Ô∏è Limitaciones y Confidencialidad

- **N√∫mero limitado de clases**: solo se trataron algunas clases, ya que la recolecci√≥n y el etiquetado de datos son procesos largos y costosos.

- **Sin comparaci√≥n con otras arquitecturas**: el proyecto no permite un benchmark directo con soluciones existentes.

- **Tecnolog√≠as no utilizadas**: no se exploraron autoencoders ni Vision Transformers, debido a limitaciones de tiempo y capacidad de c√≥mputo; adem√°s, estas arquitecturas no siempre son adecuadas para el procesamiento en **tiempo real**.

Todos los conjuntos de datos utilizados, as√≠ como el c√≥digo desarrollado en el marco de esta tesis, son **propiedad exclusiva de la empresa Othello**.  
Por razones de confidencialidad y protecci√≥n de la propiedad intelectual, no pueden compartirse p√∫blicamente.

---

## üìö Publicaciones / Art√≠culos

### üéì Tesis

- üá´üá∑ **D√©tection d‚Äôanomalies en temps r√©el dans un flux vid√©o** (2023)  
  Autor: F. Poirier  
  [Enlace HAL](https://hal.science/tel-04792952)  

- üá¨üáß **Real-Time Anomaly Detection in Video Streams** (2023)  
  Autor: F. Poirier  
  Tesis traducida  
  [Enlace HAL](https://hal.science/tel-04824941)  
  [Enlace ArXiv](https://arxiv.org/abs/2411.19731)  

### üìù Art√≠culos cient√≠ficos

- üá¨üáß **From CNN to CNN RNN Adapting Visualization Techniques for Time-Series Anomaly Detection** (2025, enviado)  
  Autores: F. Poirier, M. Lamolle  
  En proceso de env√≠o a la revista *Engineering Applications of Artificial Intelligence*

- üá¨üáß **From CNN to CNN+ RNN: Adapting Visualization Techniques for Time-Series Anomaly Detection** (2024)  
  Autor: F. Poirier  
  [Enlace ArXiv](https://arxiv.org/abs/2411.04707)  

- üá¨üáß **Real-Time Anomalies Detection on Video** (2024)  
  Autor: F. Poirier  
  [Enlace ArXiv](https://arxiv.org/abs/2410.18051)  

- üá¨üáß **Hybrid Architecture for Real-Time Video Anomaly Detection: Integrating Spatial and Temporal Analysis** (2024)  
  Autor: F. Poirier  
  [Enlace ArXiv](https://arxiv.org/abs/2410.15909)  

- üá¨üáß **Enhancing Anomaly Detection in Videos using a Combined YOLO and a VGG GRU Approach** (2023)  
  Autores: F. Poirier, R. Jaziri, C. Srour, G. Bernard  
  Conferencia: 20th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA)  
  [Enlace IEEE](https://ieeexplore.ieee.org/abstract/document/10479307)  

- üá´üá∑ **D√©tection d‚Äôanomalies en temps r√©el dans le flux vid√©o** (2022)  
  Autores: F. Poirier, R. Jaziri, C. Srour, G. Bernard  
  [Enlace HAL](https://hal.science/hal-04810781)  

### üèÜ Posters y distinciones

- üá´üá∑ **D√©tection d‚Äôanomalies en temps r√©el dans le flux vid√©o**  
  Poirier Fabien, R. Jaziri, C. Srour, G. Bernard  
  Poster presentado en EGC 2022  
  [Enlace HAL](https://hal.science/hal-04830165v1/document)  
  üèÖ **Premio a la Mejor Demostraci√≥n** EGC 2022
