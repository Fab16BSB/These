![Doctorate Thesis](https://img.shields.io/badge/Doctorate-Thesis-ffb6c1)
![University: Paris 8](https://img.shields.io/badge/University-Paris%208-red)
![deep: learning](https://img.shields.io/badge/deep-learning-blue)
![python](https://img.shields.io/badge/python-brightgreen)
![Contributors](https://img.shields.io/badge/contributor-1-orange)
![Stars](https://img.shields.io/github/stars/Fab16BSB/These?color=orange)
![Fork](https://img.shields.io/github/forks/Fab16BSB/These?color=orange)
![Watchers](https://img.shields.io/github/watchers/Fab16BSB/These?color=orange)


## ğŸŒ Versions multilingues du README

| ğŸ‡«ğŸ‡· FranÃ§ais | ğŸ‡¬ğŸ‡§ English | ğŸ‡ªğŸ‡¸ EspaÃ±ol |
|-------------|------------|------------|
| Vous Ãªtes ici ! | [README.md](./README.md) | [README.es.md](./README.es.md) |

----

# DÃ©tection dâ€™anomalies en temps rÃ©el dans un flux vidÃ©o

## ğŸ“Œ Contexte  

Jâ€™ai rÃ©alisÃ© une **thÃ¨se CIFRE** (Convention Industrielle de Formation par la Recherche) dâ€™une durÃ©e de **3 ans** (du 4 dÃ©cembre 2019 au 4 dÃ©cembre 2022), intitulÃ©e **_DÃ©tection dâ€™anomalies en temps rÃ©el dans un flux vidÃ©o_**.  

Durant cette pÃ©riode, jâ€™ai occupÃ© le poste de **chercheur** au sein de la startup [**Othello** ğŸš€](http://www.othello.group), ce qui mâ€™a permis de mener mes travaux de recherche directement en entreprise tout en Ã©tant encadrÃ© par le **Laboratoire dâ€™Intelligence Artificielle et SÃ©mantique des DonnÃ©es (LIASD)** ğŸ§  de lâ€™UniversitÃ© **Paris 8** ğŸ“š, favorisant ainsi le transfert de connaissances entre le monde industriel et la recherche ğŸ¤.

---

## ğŸ¯ Objectif  

Lâ€™objectif de ce projet Ã©tait de dÃ©velopper une solution de **dÃ©tection dâ€™anomalies par intelligence artificielle** ğŸ¤–, dans le cadre de lâ€™appel Ã  projet visant Ã  **assurer la sÃ©curitÃ© des Jeux Olympiques de Paris 2024** ğŸ….
Parmi les contraintes imposÃ©es :  
- Le modÃ¨le devait fonctionner **en temps rÃ©el** afin de permettre une intervention rapide et ainsi garantir la sÃ©curitÃ© et la sÃ»retÃ© des personnes impliquÃ©es.
  
- Les ressources disponibles Ã©taient limitÃ©es : **pas dâ€™accÃ¨s Ã  un serveur GPU** ni Ã  de **vÃ©ritables camÃ©ras de surveillance**, ce qui a renforcÃ© le dÃ©fi technique du projet.
  
- Le modÃ¨le devait Ãªtre **lÃ©ger et optimisÃ©**, de sorte Ã  pouvoir fonctionner sans nÃ©cessiter un serveur GPU puissant, ce qui Ã©tait essentiel compte tenu des ressources limitÃ©es.  

---

## ğŸ’¡ Postulat  

En tant quâ€™humains, nous pouvons facilement identifier une situation anormale dans une scÃ¨ne ou une vidÃ©o :  
- un **incendie** grÃ¢ce Ã  la prÃ©sence de fumÃ©e ou de flammes ğŸ”¥  
- une **bagarre** en observant les interactions entre plusieurs personnes ğŸ‘¥  
- un **accident de la route** en surveillant les vÃ©hicules ğŸš—  
- un **danger** potentiel en repÃ©rant une arme blanche ou Ã  feu ğŸ—¡ï¸ğŸ”«  
- De plus, un accident de la route **ne peut pas se produire** si aucun vÃ©hicule nâ€™est visible Ã  lâ€™Ã©cran.

Ces observations m'ont inspirÃ©s Ã  combiner dans ce projet :  
1. **Analyse spatiale** : dÃ©tection dâ€™objets et de poses humaines potentiellement suspectes   
2. **Analyse temporelle** : suivi des actions des objets et des individus dans le temps    

Lâ€™objectif est de crÃ©er une architecture combinant **analyse spatiale et temporelle**, reproduisant, de maniÃ¨re simplifiÃ©e, la faÃ§on dont les humains interprÃ¨tent une scÃ¨ne en observant les objets et leur dynamique pour comprendre la situation dans son ensemble dans le but dâ€™Ã©valuer si cette approche peut surpasser les mÃ©thodes classiques.

---

## âš™ï¸ Environnement technique  

L'esmble du projet a Ã©tÃ© dÃ©veloppÃ© sur un ordinateur portable fourni par lâ€™entreprise, Ã©quipÃ© des composants suivants :  
- **MÃ©moire RAM :** 32 Go  
- **Processeur :** Intel Core i9, 16 cÅ“urs cadencÃ©s Ã  2,3 GHz  
- **Carte graphique :** Nvidia GeForce RTX 2080 avec 8 Go de RAM dÃ©diÃ©e  

CÃ´tÃ© logiciel, le dÃ©veloppement et lâ€™entraÃ®nement des modÃ¨les ont Ã©tÃ© rÃ©alisÃ©s avec :  
- **Python**  
- **Keras** pour la conception et lâ€™entraÃ®nement des modÃ¨les dâ€™intelligence artificielle

---

## ğŸ“Š Jeu de donnÃ©es  

### 1ï¸âƒ£ VidÃ©os pour la dÃ©tection dâ€™anomalies 

Aucun jeu de donnÃ©es public nâ€™Ã©tait suffisamment adaptÃ© pour entraÃ®ner efficacement notre modÃ¨le :  
- Manque de volume et de variÃ©tÃ©  
- QualitÃ© insuffisante  
- PrÃ©sence dâ€™anomalies ne reprÃ©sentant pas de consÃ©quences directes sur la sÃ©curitÃ© et la sÃ»retÃ© des personnes impliquÃ©es  

Nous avons donc conÃ§u notre propre jeu de donnÃ©es.

âš ï¸ *Ce jeu de donnÃ©es est propriÃ©taire et ne peut Ãªtre ni partagÃ© ni diffusÃ©.* âš ï¸

#### ğŸ·ï¸ Classes et rÃ©partition

Le jeu de donnÃ©es est composÃ© de **4 classes** :  
- **Fight** (bagarre)  
- **Shooting** (coup de feu)  
- **Fire** (incendie)  
- **Normal** (aucun problÃ¨me dÃ©tectÃ©)  

Il est **dÃ©sÃ©quilibrÃ©**, car certaines anomalies sont plus rares que dâ€™autres. Sa rÃ©partition est la suivante :  

![RÃ©partition des classes](images/dataset_distribution.png)  

| Classe  | Train Videos (nb) | Train Duration | Validation Videos (nb) | Validation Duration |
|---------|-----------------|----------------|-----------------------|------------------------|
| Fight   | 587             | 0h50           | 391                   | 0h31                   |
| Fire    | 237             | 3h40           | 61                    | 0h46                   |
| Shooting| 247             | 0h17           | 64                    | 0h08                   |


#### ğŸ–¼ï¸ PrÃ©-traitement et augmentation  

- Chaque vidÃ©o contenant une anomalie a Ã©tÃ© **dÃ©coupÃ©e manuellement** pour ne conserver que la portion pertinente.  
- Les vidÃ©os ont Ã©tÃ© chargÃ©es via un **video generator** (voir [repo GitHub](lien_vers_repo)) pour former des **sÃ©quences de 20 images de taille 112x122**.  
- Une **data augmentation rÃ©aliste** a Ã©tÃ© appliquÃ©e pour simuler des situations plausibles :  
  - Modification de la **luminositÃ©**  
  - **Flip horizontal**  
  - **Zoom**  


### 2ï¸âƒ£ Images pour la dÃ©tection dâ€™objets  

ConformÃ©ment au postulat de **combiner dÃ©tection dâ€™objets et dÃ©tection dâ€™anomalies**, nous avons crÃ©Ã© un **second jeu de donnÃ©es basÃ© sur des images**, ciblant les entitÃ©s prÃ©sentes Ã  lâ€™Ã©cran et liÃ©es aux anomalies.  

#### ğŸ·ï¸ Classes et volume

Les classes et volumes approximatifs sont les suivants :  
- **Armes Ã  feu** : ~10 000 images ğŸ”«  
- **Flammes** : >2 000 images ğŸ”¥  
- **Personnes** : chaque individu prÃ©sent dans les images ğŸ‘¤  

Des images **ne contenant aucune de ces classes** ont Ã©galement Ã©tÃ© ajoutÃ©es, pour lâ€™apprentissage et la validation, afin de mieux gÃ©rer les situations normales.  

#### ğŸ–¼ï¸ Annotation et utilisation

- Les images ont Ã©tÃ© **Ã©tiquetÃ©es manuellement**.

---

## ğŸ§  ModÃ¨le

Le systÃ¨me de dÃ©tection combine plusieurs architectures pour traiter les flux vidÃ©o Ã  la fois sur le **plan spatial** et le **plan temporel** :  

- **CNN + RNN** : utilisÃ© pour lâ€™**analyse temporelle** des sÃ©quences vidÃ©o.  
- **YOLOv3, YOLOv4, YOLOv7** : utilisÃ©s pour lâ€™**analyse spatiale** des images.  

![SchÃ©ma du systÃ¨me](images/these_architecture1.png)  
![SchÃ©ma du systÃ¨me](images/these_architecture2.png)  

### ğŸ”„ Combinaison des modÃ¨les

Les modÃ¨les peuvent Ãªtre combinÃ©s de diffÃ©rentes maniÃ¨res :  
1. **SÃ©rie** : YOLO est utilisÃ© pour un **prÃ©-traitement spatial**, et ses rÃ©sultats sont ensuite transmis au CNN+RNN pour lâ€™analyse temporelle.
   ![SchÃ©ma du systÃ¨me](images/modele_serie.png)
   
2. **ParallÃ¨le** : les deux modÃ¨les effectuent simultanÃ©ment leur dÃ©tection, puis leurs rÃ©sultats sont **fusionnÃ©s** pour produire la prÃ©diction finale.  
![SchÃ©ma du systÃ¨me](images/modele_parallele.png)

Un **module dâ€™explicabilitÃ©** est Ã©galement intÃ©grÃ© pour analyser certaines prÃ©dictions, bien quâ€™il **ne fonctionne pas en temps rÃ©el**.  


### ğŸ–¥ CNN + RNN
![SchÃ©ma CNN + RNN](images/modele.png)  

- **CNN** : VGG19  
- **RNN** : GRU avec **1024 neurones**,
- suivi dâ€™un **MLP Ã  3 couches** (1024 â†’ 512 â†’ 128 neurones)  
  - **Dropout :** 50% sur toutes les couches  
  - **RÃ©gularisation L2 :** coefficient 0.01  
- **EntraÃ®nement** :  
  - Batch training sur nos vidÃ©os  
  - **200 epochs**  
  - Optimiseur : **Stochastic Gradient Descent (SGD)** avec **learning rate 0.01**


### ğŸ–¼ YOLO

Les modÃ¨les **YOLOv3, YOLOv4 et YOLOv7** ont Ã©tÃ© **re-entraÃ®nÃ©s sur notre jeu de donnÃ©es dâ€™images**, permettant une dÃ©tection adaptÃ©e aux classes dÃ©finies dans notre dataset.

---

## ğŸ“ˆ RÃ©sultats

### ğŸ–¼ï¸ 1. DÃ©tection dâ€™objets

Les rÃ©sultats suivants montrent la dÃ©tection des objets clÃ©s sur des images extraites de nos vidÃ©os.
Ces rÃ©sultats illustrent la capacitÃ© de notre modÃ¨le YOLO Ã  localiser prÃ©cisÃ©ment les entitÃ©s importantes pour la sÃ©curitÃ©.

![Exemple dÃ©tection d'arme Ã  feu](images/yolo_result.png)
![Exemple dÃ©tection de flamme](images/yolo_result_2.png)

---

### ğŸ¬ 2. Analyse vidÃ©o

Pour illustrer les performances de notre approche, nous prÃ©sentons ci-dessous plusieurs exemples dâ€™analyses vidÃ©o.  
Chaque sÃ©quence combine la **dÃ©tection spatiale** (personnes, objets, flammes, etc.) avec une **analyse temporelle** basÃ©e sur VGG-GRU, permettant de mettre en Ã©vidence les anomalies en temps rÃ©el.  

ğŸ“¸ **captures dâ€™Ã©cran**  
- DÃ©tection dâ€™un dÃ©part de feu.  
![Exemple dÃ©tection d'incendie (image)](images/incendie.png)  

- DÃ©tection dâ€™un accident de la route.  
![Exemple dÃ©tection d'accident (image)](images/accident.png)  

ğŸ¥ **Exemples vidÃ©os**  
- [â–¶ï¸ DÃ©tection dâ€™incendie Ã  Notre-Dame (voir la vidÃ©o)](https://youtu.be/Zg4AAycii1M)  
- [â–¶ï¸ DÃ©tection d'une bagarre lors d'un match de rugby (voir la vidÃ©o)](https://youtu.be/vdMqYTSrXok) 

---

### ğŸ† 3. Prix de la meilleure dÃ©monstration

VidÃ©o illustrant nos premier rÃ©sultat ayant reÃ§u le **prix de la meilleure dÃ©monstration Ã  EGC 2022**.  

â–¶ï¸ [Voir la vidÃ©o de dÃ©monstration](https://www.youtube.com/watch?v=EGHUEPMI4c8)

---

### ğŸ§ ExplicabilitÃ© du modÃ¨le  

Afin de mieux comprendre le fonctionnement interne de notre systÃ¨me, nous avons intÃ©grÃ© un **module dâ€™explicabilitÃ©**.  
Celui-ci joue un double rÃ´le :  
- **Analyser les prÃ©dictions** du modÃ¨le pour en identifier les raisons.  
- **Aider au rÃ©glage des hyperparamÃ¨tres** en mettant en Ã©vidence les zones ou caractÃ©ristiques les plus influentes.  

---

#### ğŸ” Visualisation des prÃ©dictions  
Ces exemples illustrent la maniÃ¨re dont lâ€™explicabilitÃ© met en Ã©vidence les rÃ©gions pertinentes pour la dÃ©tection dâ€™anomalies :  

![Exemple explicabilitÃ©](images/explicabilite.png)  
![Exemple explicabilitÃ©](images/explicabilite_2.png)  

---

#### âš™ï¸ Aide Ã  la configuration du modÃ¨le  
Lâ€™explicabilitÃ© a Ã©galement Ã©tÃ© utilisÃ©e pour analyser les comportements des diffÃ©rentes couches (GRU et MLP), permettant dâ€™affiner la conception de lâ€™architecture et son paramÃ©trage.  

![ExplicabilitÃ© GRU](images/explicabilite_gru.png)  
![ExplicabilitÃ© MLP](images/explicabilite_mlp.png)  

---

### ğŸ“Š 5. Statistiques et performances  

Pour Ã©valuer notre approche, nous avons testÃ© diffÃ©rentes **architectures de combinaison** entre YOLO (pour lâ€™analyse spatiale) et VGG-GRU (pour lâ€™analyse temporelle).  
Deux configurations principales ont Ã©tÃ© comparÃ©es :  

1. **Mode sÃ©rie** : les sorties de YOLO servent directement dâ€™entrÃ©e au VGG-GRU.  
2. **Mode parallÃ¨le** : YOLO et VGG-GRU rÃ©alisent leurs prÃ©dictions sÃ©parÃ©ment, puis leurs rÃ©sultats sont combinÃ©s pour produire la dÃ©cision finale.  

Les tableaux suivants prÃ©sentent les performances obtenues pour ces deux configurations, en termes de **prÃ©cision, rappel, F1-score et matrice de confusion**.


#### Performance de YOLO + VGG-GRU disposÃ© en sÃ©rie

| Metric     | Accuracy | PrÃ©cision | Rappel  | F1-Score |
|------------|----------|-----------|---------|----------|
| Valeur     | 87.3%    | 87.6%     | 87.3%   | 87.1%    |


#### Matrice de confusion en pourcentage pour une Ã©valuation en sÃ©rie

| Truth \ Predicted | Bagarre | Coup de feu | Incendie | Normal |
|-------------------|---------|-------------|----------|--------|
| **Bagarre**       | 60.5%   | 2.4%        | 1.3%     | 35.8%  |
| **Coup de feu**   | 10%     | 55.6%       | 14.8%    | 19.6%  |
| **Incendie**      | 15.5%   | 10.6%       | 48%      | 25.9%  |
| **Normal**        | 3.4%    | 0.6%        | 1%       | 95%    |

#### Performance de YOLO + VGG-GRU disposÃ© en parallÃ¨le

| Metric     | Accuracy | PrÃ©cision | Rappel  | F1-Score |
|------------|----------|-----------|---------|----------|
| Valeur     | 78.42%   | 85.60%    | 78.42%  | 81.16%   |

####  Matrice de confusion en pourcentage pour une Ã©valuation en parallÃ¨le

| Truth \ Predicted | Bagarre | Coup de feu | Incendie | Normal |
|-------------------|---------|-------------|----------|--------|
| **Bagarre**       | 63.66%  | 6.58%       | 1.93%    | 27.83% |
| **Coup de feu**   | 9.94%   | 66.06%      | 9.33%    | 14.67% |
| **Incendie**      | 13.66%  | 15.73%      | 57.71%   | 12.9%  |
| **Normal**        | 7.43%   | 5.96%       | 3.98%    | 82.63% |



Enfin, nous avons Ã©valuÃ© les deux approches (parallÃ¨le et sÃ©rie) du point de vue **temps dâ€™exÃ©cution** afin de vÃ©rifier leur applicabilitÃ© en conditions rÃ©elles.  
Les rÃ©sultats montrent que :  
- le **mode parallÃ¨le** permet un traitement plus rapide, avec un temps moyen proche du temps rÃ©el pour certaines vidÃ©os,  
- le **mode sÃ©rie**, bien que plus prÃ©cis, entraÃ®ne un coÃ»t temporel nettement plus Ã©levÃ©.  

Les tableaux ci-dessous rÃ©sument ces mesures pour diffÃ©rentes vidÃ©os dâ€™Ã©valuation.  


#### Temps dâ€™exÃ©cution de YOLO + VGG-GRU en parallÃ¨le

| DurÃ©e de la vidÃ©o | FPS de la vidÃ©o | Moyenne de dÃ©tections | Temps de traitement |
|-------------------|-----------------|-----------------------|---------------------|
| 16s               | 33              | 601ms                 | 15s                 |
| 44s               | 30              | 533ms                 | 35s                 |
| 9s                | 30              | 994ms                 | 12s                 |
| 35s               | 30              | 1.1s                  | 57s                 |
| 23s               | 30              | 1s06                  | 35s                 |
| 1min 43           | 30              | 758ms                 | 116s (1min 56)      |
| 50s               | 30              | 826ms                 | 61s                 |
| 1min 05           | 30              | 886ms                 | 83s (1min 23)       |
| 2s                | 30              | 847ms                 | 847ms               |
| 9s                | 30              | 870ms                 | 11s                 |
| 2s                | 30              | 1s                    | 1s                  |


#### Temps dâ€™exÃ©cution de YOLO + VGG-GRU en sÃ©rie

| DurÃ©e de la vidÃ©o | FPS de la vidÃ©o | Moyenne de dÃ©tections | Temps de traitement |
|-------------------|-----------------|-----------------------|---------------------|
| 16s               | 33              | 1s                    | 26s                 |
| 44s               | 30              | 1s                    | 71s (1min 11)       |
| 9s                | 30              | 1.5s                  | 20s                 |
| 35s               | 30              | 1.5s                  | 81s (1min 21)       |
| 23s               | 30              | 1.5s                  | 48s                 |
| 1min 43           | 30              | 1.2s                  | 193s (3min 13)      |
| 50s               | 30              | 1.3s                  | 102s (1min 42)      |
| 1min 05           | 30              | 1.4s                  | 134s (2min 14)      |
| 2s                | 30              | 1.3s                  | 1.3s                |
| 9s                | 30              | 1.3s                  | 17s                 |
| 2s                | 30              | 1.5s                  | 1.5s                |


---

## âš ï¸ Limitations & ConfidentialitÃ©

- **Nombre limitÃ© de classes** : seules quelques classes ont Ã©tÃ© traitÃ©es, car la collecte et la labellisation des donnÃ©es sont longues et coÃ»teuses.
  
- **Pas de comparaison avec dâ€™autres architectures** : le projet ne permet pas de benchmark direct avec des solutions existantes.
  
- **Technologies non utilisÃ©es** : les auto-encodeurs ou Vision Transformers nâ€™ont pas Ã©tÃ© explorÃ©s, faute de temps et de puissance, de plus ces architectures ne sont pas toujours adaptÃ©es au traitement en **temps rÃ©el**.


Lâ€™ensemble des jeux de donnÃ©es utilisÃ©s ainsi que le code dÃ©veloppÃ© dans le cadre de cette thÃ¨se sont la **propriÃ©tÃ© exclusive de lâ€™entreprise Othello**.  
Pour des raisons de confidentialitÃ© et de protection de la propriÃ©tÃ© intellectuelle, ils ne peuvent donc pas Ãªtre partagÃ©s publiquement.  

---

## ğŸ“š Publications / Articles

### ğŸ“ ThÃ¨ses

- ğŸ‡«ğŸ‡· **DÃ©tection dâ€™anomalies en temps rÃ©el dans un flux vidÃ©o** (2023)  
  Auteur : F. Poirier  
  [Lien HAL](https://hal.science/tel-04792952)  

- ğŸ‡¬ğŸ‡§ **Real-Time Anomaly Detection in Video Streams** (2023)  
  Auteur : F. Poirier  
  ThÃ¨se traduite  
  [Lien HAL](https://hal.science/tel-04824941)
  [Lien ArXiv](https://arxiv.org/abs/2411.19731)  



### ğŸ“ Articles scientifiques

- ğŸ‡¬ğŸ‡§ **From CNN to CNN RNN Adapting Visualization Techniques for Time-Series Anomaly Detection** (2025, soumis)  
  Auteurs : F. Poirier, M. Lamolle  
  En cours de soumission au journal *Engineering Applications of Artificial Intelligence*
  
- ğŸ‡¬ğŸ‡§ **From CNN to CNN+ RNN: Adapting Visualization Techniques for Time-Series Anomaly Detection** (2024)  
  Auteur : F. Poirier  
  [Lien ArXiv](https://arxiv.org/abs/2411.04707)  

- ğŸ‡¬ğŸ‡§ **Real-Time Anomalies Detection on Video** (2024)  
  Auteur : F. Poirier  
  [Lien ArXiv](https://arxiv.org/abs/2410.18051)  

- ğŸ‡¬ğŸ‡§ **Hybrid Architecture for Real-Time Video Anomaly Detection: Integrating Spatial and Temporal Analysis** (2024)  
  Auteur : F. Poirier  
  [Lien ArXiv](https://arxiv.org/abs/2410.15909)  

- ğŸ‡¬ğŸ‡§ **Enhancing Anomaly Detection in Videos using a Combined YOLO and a VGG GRU Approach** (2023)  
  Auteurs : F. Poirier, R. Jaziri, C. Srour, G. Bernard  
  ConfÃ©rence : 20th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA)  
  [Lien IEEE](https://ieeexplore.ieee.org/abstract/document/10479307)  

- ğŸ‡«ğŸ‡· **DÃ©tection dâ€™anomalies en temps rÃ©el dans le flux vidÃ©o** (2022)  
  Auteurs : F. Poirier, R. Jaziri, C. Srour, G. Bernard  
  [Lien HAL](https://hal.science/hal-04810781)  



### ğŸ† Posters et distinctions

- ğŸ‡«ğŸ‡· **DÃ©tection dâ€™anomalies en temps rÃ©el dans le flux vidÃ©o**  
  Poirier Fabien, R. Jaziri, C. Srour, G. Bernard  
  Poster prÃ©sentÃ© Ã  EGC 2022  
  [Lien HAL](https://hal.science/hal-04830165v1/document)  
  ğŸ… **Prix de la Meilleure DÃ©monstration** EGC 2022
